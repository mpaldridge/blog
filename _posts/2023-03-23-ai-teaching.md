---
layout: post
title:  "Notes from a session on AI and teaching"
date:   2023-03-29
categories: writing
permalink: /blog/ai-teaching.html
---

This morning, I went to a two-hour session with the slightly cumbersome title [Assessment Matters: Artificial Intelligence and Assessment – Challenges and Opportunities](https://teachingexcellence.leeds.ac.uk/events/assessment-matters-artificial-intelligence-and-assessment-challenges-and-opportunities/) at the university. Four speakers gave 10-to-15-minute talks on aspects of AI and teaching, the organiser made some comments, and we had some smaller-group discussion time. (I also had a very useful chat with a maths colleague on the walk back.) What follows is a slightly neatened-up version of the notes I made; a mixture of the thoughts of speakers, other attendees, and myself.

(Since this wasn't a public event, I think I'll observe the Chatham House rule and not name individuals who made comments.)

* As a self-selecting audience (of about 90), it's perhaps not surprising that "AI is going to change a lot of stuff, very quickly; this is mostly good, but we will have some difficulties" was something close to a unanimous view in the room. And my view too. But I think "Big deal! AI is just fancy autocomplete" and "AI is a very bad thing for education and must be stopped" represent quite a large proportion of colleagues who don't sign up to a voluntary AI-in-education meeting, and it would have been better for the intellectual diversity of the event if we'd had some of them around.
* As far as the majority of people in the meeting were concerned, at the moment "AI", in education terms, means "ChatGPT" (to the annoyance of one more technically experienced member of my discussion group). I expect this may change when the new ["Copilot"](https://www.theverge.com/2023/3/17/23644501/microsoft-copilot-ai-office-documents-microsoft-365-report) is added to Microsoft Office.
* Speaker: "The biggest innovation in ChatGPT was not its qualities as a language model but its usability."
* There was a general feeling that the kids are all learning the best AI-cheating tips from TikTok while we stuck-in-the-mud academics are already falling hopelessly behind. This is actually the opposite to my experience – my nerdy maths colleagues are all rather interested in this, but when I mention ChatGPT or related things with students, they rarely have any idea what I'm talking about.
* A speaker commented that "students cheat on essays using AI" might be less common than "essay mills get vastly more productive – so cheaper and better – using AI, and students cheat using essay mill".
* Many attendees thought [our policy on proof-reading](https://www.leeds.ac.uk/secretariat/documents/proof_reading_policy.pdf) is already confused and bad. Do we need a wholesale redesign of our academic integrity policies, not merely an AI add-on?
* One speaker spoke of experiments in "AI detection software" which showed good results, even when simple techniques were used to try and defeat the detection software. But there was widespread scepticism in the room – the consensus was that AI detection is a losing battle and it would be a waste to invest time and effort into something that will only be helpful in the very short term. (My comment was: Even if detection software *does* work, are we really going to expel students on the grounds of "TurnItIn says there's a 99% chance an AI wrote this"? That's quite different from finding a textbook where the same eight paragraphs appear word-for-word the same.)
* The speaker who seemed to know the most about how language models actually work talked about how questions involving counting, calculation and logical deduction very often defeat the language-model AIs, so he will be increasing the proportion of coursework questions that involve those skills. A questioner (representative a fairly common view in the room, I think) put it to the speaker that the language models will surely get better at these soon. The speaker didn't think so; he predicts they will get "wider not deeper" -- better at non-English languages, better at formats that aren't just text, but not fundamentally change the way it "thinks". After the past couple of years, I am very reluctant to say "AI won't be able to do *X*", but he's the expert, and I take that view seriously.
* The session was on "Artificial Intelligence and Assessment", but it's not *assessment* I'm most concerned about, personally. In maths, most of our modules have the majority of their marks coming from an in-person proctored exam, and many of our report/dissertation modules have some sort of brief oral exam, so these assessments seem pretty AI-proof. One speaker dismissed proctored exams and oral exams pretty much out of hand as "not inclusive". This strikes me as far too simplistic: first, inclusivity is more complicated than an "inclusive"/"not inclusive" binary; second, many people – say, those who struggle to find uninterupted quiet period of times at home, who find it tiring to read off computer screens, who find writing physically difficult, etc – might find a proctored or oral exam more inclusive than other methods. If money and exam-hall space were no object, I would like my exams to have a more generous time limits -- I'd be happy for students to have taken, say, five hours over my currently-120-minute exam, if that's what they wanted to take up all that time; in addition, this means students who take their exam elsewhere with extended time could take their exam in the same hall with everyone else. But I'm getting off-topic now...
